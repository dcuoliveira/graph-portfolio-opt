{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n",
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch.utils.data as data\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "from utils.dataset_utils import concatenate_prices_returns, create_rolling_window_ts, timeseries_train_test_split\n",
    "from loss_functions.SharpeLoss import SharpeLoss\n",
    "from models.DLPO import DLPO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# neural network hyperparameters\n",
    "input_size = 4 * 2\n",
    "output_size = 4\n",
    "hidden_size = 64\n",
    "num_layers = 1\n",
    "\n",
    "# optimization hyperparameters\n",
    "learning_rate = 1e-3\n",
    "\n",
    "# training hyperparameters\n",
    "device = torch.device('cpu')\n",
    "epochs = 1000\n",
    "batch_size = 10\n",
    "drop_last = True\n",
    "num_timesteps_in = 50\n",
    "num_timesteps_out = 1\n",
    "test_ratio = 0.2\n",
    "ascent = True\n",
    "\n",
    "# relevant paths\n",
    "source_path = os.getcwd()\n",
    "inputs_path = os.path.join(source_path, \"data\", \"inputs\")\n",
    "\n",
    "# prepare dataset\n",
    "prices = pd.read_excel(os.path.join(inputs_path, \"etfs-zhang-zohren-roberts.xlsx\"))\n",
    "prices.set_index(\"date\", inplace=True)\n",
    "names = prices.columns\n",
    "returns = np.log(prices).diff().dropna()\n",
    "prices = prices.loc[returns.index]\n",
    "features, names = concatenate_prices_returns(prices=prices, returns=returns)\n",
    "idx = features.index\n",
    "returns = returns[names].loc[idx].values.astype('float32')\n",
    "prices = prices[names].loc[idx].values.astype('float32')\n",
    "features = features.loc[idx].values.astype('float32')  \n",
    "\n",
    "# define train and test datasets\n",
    "X_train, X_test, prices_train, prices_test = timeseries_train_test_split(features, prices, test_ratio=test_ratio)\n",
    "X_train, X_val, prices_train, prices_val = timeseries_train_test_split(X_train, prices_train, test_ratio=test_ratio) \n",
    "\n",
    "X_train, prices_train = create_rolling_window_ts(features=X_train, \n",
    "                                                 target=prices_train,\n",
    "                                                 num_timesteps_in=num_timesteps_in,\n",
    "                                                 num_timesteps_out=num_timesteps_out)\n",
    "X_val, prices_val = create_rolling_window_ts(features=X_val, \n",
    "                                             target=prices_val,\n",
    "                                             num_timesteps_in=num_timesteps_in,\n",
    "                                             num_timesteps_out=num_timesteps_out)\n",
    "X_test, prices_test = create_rolling_window_ts(features=X_test, \n",
    "                                               target=prices_test,\n",
    "                                               num_timesteps_in=num_timesteps_in,\n",
    "                                               num_timesteps_out=num_timesteps_out)\n",
    "\n",
    "# define data loaders\n",
    "train_loader = data.DataLoader(data.TensorDataset(X_train, prices_train), shuffle=False, batch_size=batch_size, drop_last=drop_last)\n",
    "val_loader = data.DataLoader(data.TensorDataset(X_val, prices_val), shuffle=False, batch_size=batch_size, drop_last=drop_last)\n",
    "test_loader = data.DataLoader(data.TensorDataset(X_test, prices_test), shuffle=False, batch_size=batch_size, drop_last=drop_last)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 10, Train sharpe : 4.31024, Train sharpe : -0.18936:   1%|          | 11/1001 [02:01<3:02:08, 11.04s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 44\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[1;32m     42\u001b[0m     \u001b[39mfor\u001b[39;00m X_batch, prices_batch \u001b[39min\u001b[39;00m test_loader:\n\u001b[1;32m     43\u001b[0m         \u001b[39m# compute forward propagation\u001b[39;00m\n\u001b[0;32m---> 44\u001b[0m         weights_pred \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mforward(X_batch)\n\u001b[1;32m     46\u001b[0m         \u001b[39m# compute loss\u001b[39;00m\n\u001b[1;32m     47\u001b[0m         loss \u001b[39m=\u001b[39m lossfn(prices_batch, weights_pred, ascent\u001b[39m=\u001b[39mascent)\n",
      "File \u001b[0;32m~/Documents/Daniel/codes/graph-portfolio-opt/src/models/DLPO.py:67\u001b[0m, in \u001b[0;36mDLPO.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     64\u001b[0m c0 \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mzeros(D \u001b[39m*\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_layers, batch_size, hcell)\n\u001b[1;32m     66\u001b[0m \u001b[39m# propagate input through LSTM\u001b[39;00m\n\u001b[0;32m---> 67\u001b[0m o1t, (ht, _) \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlstm(x, (h0, c0))\n\u001b[1;32m     69\u001b[0m \u001b[39m# linear aggregate hidden states to match output\u001b[39;00m\n\u001b[1;32m     70\u001b[0m o2t \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlinear(o1t)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/graph-popt/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/graph-popt/lib/python3.10/site-packages/torch/nn/modules/rnn.py:810\u001b[0m, in \u001b[0;36mLSTM.forward\u001b[0;34m(self, input, hx)\u001b[0m\n\u001b[1;32m    806\u001b[0m     \u001b[39m# Each batch of the hidden state should match the input sequence that\u001b[39;00m\n\u001b[1;32m    807\u001b[0m     \u001b[39m# the user believes he/she is passing in.\u001b[39;00m\n\u001b[1;32m    808\u001b[0m     hx \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpermute_hidden(hx, sorted_indices)\n\u001b[0;32m--> 810\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcheck_forward_args(\u001b[39minput\u001b[39;49m, hx, batch_sizes)\n\u001b[1;32m    811\u001b[0m \u001b[39mif\u001b[39;00m batch_sizes \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    812\u001b[0m     result \u001b[39m=\u001b[39m _VF\u001b[39m.\u001b[39mlstm(\u001b[39minput\u001b[39m, hx, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_flat_weights, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbias, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_layers,\n\u001b[1;32m    813\u001b[0m                       \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdropout, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtraining, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbidirectional, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbatch_first)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/graph-popt/lib/python3.10/site-packages/torch/nn/modules/rnn.py:731\u001b[0m, in \u001b[0;36mLSTM.check_forward_args\u001b[0;34m(self, input, hidden, batch_sizes)\u001b[0m\n\u001b[1;32m    725\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcheck_forward_args\u001b[39m(\u001b[39mself\u001b[39m,  \u001b[39m# type: ignore[override]\u001b[39;00m\n\u001b[1;32m    726\u001b[0m                        \u001b[39minput\u001b[39m: Tensor,\n\u001b[1;32m    727\u001b[0m                        hidden: Tuple[Tensor, Tensor],\n\u001b[1;32m    728\u001b[0m                        batch_sizes: Optional[Tensor],\n\u001b[1;32m    729\u001b[0m                        ):\n\u001b[1;32m    730\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcheck_input(\u001b[39minput\u001b[39m, batch_sizes)\n\u001b[0;32m--> 731\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcheck_hidden_size(hidden[\u001b[39m0\u001b[39m], \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mget_expected_hidden_size(\u001b[39minput\u001b[39;49m, batch_sizes),\n\u001b[1;32m    732\u001b[0m                            \u001b[39m'\u001b[39m\u001b[39mExpected hidden[0] size \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m, got \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m'\u001b[39m)\n\u001b[1;32m    733\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcheck_hidden_size(hidden[\u001b[39m1\u001b[39m], \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_expected_cell_size(\u001b[39minput\u001b[39m, batch_sizes),\n\u001b[1;32m    734\u001b[0m                            \u001b[39m'\u001b[39m\u001b[39mExpected hidden[1] size \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m, got \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m'\u001b[39m)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/graph-popt/lib/python3.10/site-packages/torch/nn/modules/rnn.py:226\u001b[0m, in \u001b[0;36mRNNBase.get_expected_hidden_size\u001b[0;34m(self, input, batch_sizes)\u001b[0m\n\u001b[1;32m    224\u001b[0m     mini_batch \u001b[39m=\u001b[39m \u001b[39mint\u001b[39m(batch_sizes[\u001b[39m0\u001b[39m])\n\u001b[1;32m    225\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 226\u001b[0m     mini_batch \u001b[39m=\u001b[39m \u001b[39minput\u001b[39;49m\u001b[39m.\u001b[39;49msize(\u001b[39m0\u001b[39;49m) \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbatch_first \u001b[39melse\u001b[39;00m \u001b[39minput\u001b[39m\u001b[39m.\u001b[39msize(\u001b[39m1\u001b[39m)\n\u001b[1;32m    227\u001b[0m num_directions \u001b[39m=\u001b[39m \u001b[39m2\u001b[39m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbidirectional \u001b[39melse\u001b[39;00m \u001b[39m1\u001b[39m\n\u001b[1;32m    228\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mproj_size \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# (1) model\n",
    "model = DLPO(input_size=input_size,\n",
    "             output_size=output_size,\n",
    "             hidden_size=hidden_size,\n",
    "             num_layers=num_layers,\n",
    "             batch_first=True,\n",
    "             num_timesteps_out=num_timesteps_out).to(device)\n",
    "\n",
    "# (2) loss fucntion\n",
    "lossfn = SharpeLoss()\n",
    "\n",
    "# (3) optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# (4) training procedure\n",
    "training_loss_values = test_loss_values = []\n",
    "pbar = tqdm(range(epochs + 1), total=(epochs + 1))\n",
    "for epoch in pbar:\n",
    "\n",
    "    # train model\n",
    "    model.train()\n",
    "    for X_batch, prices_batch in train_loader:\n",
    "                \n",
    "        # compute forward propagation\n",
    "        # NOTE - despite num_timesteps_out=1, the predictions are being made on the batch_size(=10) dimension. Need to fix that.\n",
    "        weights_pred = model.forward(X_batch)\n",
    "\n",
    "        # compute loss\n",
    "        loss = lossfn(prices_batch, weights_pred, ascent=ascent)\n",
    "\n",
    "        # compute gradients and backpropagate\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        training_loss_values.append(loss.detach().item() * -1)\n",
    "\n",
    "    train_loss = (loss.detach().item() * -1)\n",
    "\n",
    "    # evaluate model \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for X_batch, prices_batch in test_loader:\n",
    "            # compute forward propagation\n",
    "            weights_pred = model.forward(X_batch)\n",
    "\n",
    "            # compute loss\n",
    "            loss = lossfn(prices_batch, weights_pred, ascent=ascent)\n",
    "            test_loss_values.append(loss.detach().item() * -1)\n",
    "\n",
    "    test_loss = (loss.detach().item() * -1)\n",
    "\n",
    "    pbar.set_description(\"Epoch: %d, Train sharpe : %1.5f, Train sharpe : %1.5f\" % (epoch, train_loss, test_loss))\n",
    "\n",
    "\n",
    "training_loss_df = pd.DataFrame(training_loss_values, columns=[\"sharpe_ratio\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Test sharpe (loss): 0.24907: 100%|██████████| 83/83 [00:01<00:00, 76.38it/s]\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "\n",
    "# Store for analysis\n",
    "weights = []\n",
    "prices = []\n",
    "\n",
    "pbar = tqdm(enumerate(test_loader), total=len(test_loader))\n",
    "for i, (X_batch, prices_batch) in pbar:\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # compute forward propagation\n",
    "    weights_pred = model.forward(X_batch)\n",
    "\n",
    "    # compute loss\n",
    "    loss = lossfn(prices_batch, weights_pred, ascent=True)\n",
    "    \n",
    "    # compute gradients and backpropagate\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    pbar.set_description(\"Test sharpe (loss): %1.5f\" % (loss.item() * -1))\n",
    "\n",
    "    # store predictions and true values\n",
    "    prices.append(prices_batch)\n",
    "    weights.append(weights_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gnn-popt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
